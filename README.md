# ğŸ• Sushi-Steak Classifier Using PyTorch  

Welcome to the **Pizza-Sushi-Steak Classifier**! This project demonstrates how to use **PyTorch** and its pretrained models to classify images of **pizza**, **sushi**, and **steak** with a small dataset of just 300 images.  

---

## ğŸš€ Project Overview  

The goal of this project was to identify the best model architecture for classifying images into these three food categories using a highly constrained dataset:  
- **Dataset Size**:  
  - **225 images** for training  
  - **75 images** for testing  

Through experimentation, we evaluated multiple models, including **ResNet**, **EfficientNet**, and **Vision Transformers (ViT)**.  

---

## ğŸ”‘ Key Features  

- **Pretrained PyTorch Models**: Utilized ResNet, EfficientNet, and Vision Transformers to leverage transfer learning.  
- **Feature Extraction**: Explored the effectiveness of using pretrained layers without retraining the full model.  
- **Compact Dataset**: Tackled the challenges of training on a small dataset.  
- **Vision Transformers (ViT)**: Achieved the best results using **ViT-B_32** and **ViT-B_16** as feature extractors.  

---

## ğŸ§ª Models Explored  

### 1ï¸âƒ£ **ResNet Series**  
- **Tested Models**: ResNet18, ResNet34, ResNet50, ResNet101, ResNet152  
- **Outcome**: Limited results due to dataset size and hardware constraints.  

### 2ï¸âƒ£ **EfficientNet Series**  
- **Tested Models**: EfficientNet-B0, B1, B2  
- **Outcome**: EfficientNet-B2 (Feature Extractor) delivered reasonable results.  

### 3ï¸âƒ£ **Vision Transformers (ViT)**  
- **Tested Models**: ViT-B_32, ViT-B_16  
- **Outcome**: As feature extractors, they delivered the **best performance**, showing the power of transformer-based architectures in computer vision.  

---

## ğŸ“‚ Project Structure  

```plaintext  
root/  
â”œâ”€â”€ dataset/         # Contains training and testing images  
â”œâ”€â”€ models/          # Saved PyTorch models  
â”œâ”€â”€ README.md        # Project documentation (this file)  
```

---

## ğŸ§‘â€ğŸ’» Results  

- **Best Model**: Vision Transformer (ViT-B_16 as a feature extractor)  
- **Accuracy**: Achieved 85% accuracy on the test set.  
- **Inference Time**: Lightweight and fast for real-time applications.  

---

## ğŸŒŸ Highlights  

- **Transformers in Vision**: Demonstrates how Vision Transformers can outperform traditional CNNs on small datasets.  
- **Open for Improvement**: This project is designed to be a starting pointâ€”fine-tune the models, explore other architectures, or expand the dataset.  

---

## ğŸ¤ Contributions  

Contributions are welcome! If youâ€™d like to improve this project, please fork the repository and create a pull request.  

---

## ğŸ“œ License  

This project is licensed under the **MIT License**. See the [LICENSE](./LICENSE) file for details.  

---

## ğŸ’¬ Letâ€™s Connect  

If you found this project useful or have any questions, feel free to reach out or share your feedback:  
- **LinkedIn**: [Jagannath](www.linkedin.com/in/jaganfoundr)  
- **GitHub**: [JaganFoundr](https://github.com/JaganFoundr)  
